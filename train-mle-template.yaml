# PIPELINE DEFINITION
# Name: mlops-mle-template
# Inputs:
#    labels: dict
#    location: str
#    model_description: str
#    model_name: str
#    name_bucket: str
#    path_bucket: str
#    project: str
# Outputs:
#    testing-model-metrics: system.Metrics
#    train-model-metrics: system.Metrics
components:
  comp-condition-1:
    dag:
      tasks:
        create-custom-predict:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-create-custom-predict
          inputs:
            parameters:
              labels:
                componentInputParameter: pipelinechannel--labels
              location:
                componentInputParameter: pipelinechannel--location
              name_bucket:
                componentInputParameter: pipelinechannel--name_bucket
              project:
                componentInputParameter: pipelinechannel--project
          taskInfo:
            name: Create Custom Predict Image
        upload-to-model-registry:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-upload-to-model-registry
          dependentTasks:
          - create-custom-predict
          inputs:
            artifacts:
              input_model:
                componentInputArtifact: pipelinechannel--train-model-model
            parameters:
              description:
                componentInputParameter: pipelinechannel--model_description
              labels:
                componentInputParameter: pipelinechannel--labels
              location:
                componentInputParameter: pipelinechannel--location
              model_name:
                componentInputParameter: pipelinechannel--model_name
              name_bucket:
                componentInputParameter: pipelinechannel--name_bucket
              path_bucket:
                componentInputParameter: pipelinechannel--path_bucket
              project:
                componentInputParameter: pipelinechannel--project
              serving_container_image_uri:
                taskOutputParameter:
                  outputParameterKey: Output
                  producerTask: create-custom-predict
          taskInfo:
            name: Save Model
    inputDefinitions:
      artifacts:
        pipelinechannel--train-model-model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
      parameters:
        pipelinechannel--labels:
          parameterType: STRUCT
        pipelinechannel--location:
          parameterType: STRING
        pipelinechannel--model_description:
          parameterType: STRING
        pipelinechannel--model_name:
          parameterType: STRING
        pipelinechannel--name_bucket:
          parameterType: STRING
        pipelinechannel--path_bucket:
          parameterType: STRING
        pipelinechannel--project:
          parameterType: STRING
        pipelinechannel--testing-model-Output:
          parameterType: NUMBER_DOUBLE
  comp-create-custom-predict:
    executorLabel: exec-create-custom-predict
    inputDefinitions:
      parameters:
        labels:
          parameterType: STRUCT
        location:
          parameterType: STRING
        name_bucket:
          parameterType: STRING
        project:
          parameterType: STRING
    outputDefinitions:
      parameters:
        Output:
          parameterType: STRING
  comp-get-data:
    executorLabel: exec-get-data
    outputDefinitions:
      artifacts:
        dataset:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-split-data:
    executorLabel: exec-split-data
    inputDefinitions:
      artifacts:
        data_input:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
    outputDefinitions:
      artifacts:
        data_test:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        data_train:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        scaler:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
  comp-testing-model:
    executorLabel: exec-testing-model
    inputDefinitions:
      artifacts:
        data_test:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
    outputDefinitions:
      artifacts:
        metrics:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
      parameters:
        Output:
          parameterType: NUMBER_DOUBLE
  comp-train-model:
    executorLabel: exec-train-model
    inputDefinitions:
      artifacts:
        data_train:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        scaler:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
      parameters:
        name_bucket:
          parameterType: STRING
        path_bucket:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        metrics:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
        model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
  comp-upload-to-model-registry:
    executorLabel: exec-upload-to-model-registry
    inputDefinitions:
      artifacts:
        input_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
          description: The trained model to be uploaded.
      parameters:
        credential_dict:
          description: A dictionary containing the service account credentials.
          isOptional: true
          parameterType: STRUCT
        description:
          description: A description for the uploaded model. Defaults to None.
          isOptional: true
          parameterType: STRING
        labels:
          description: A dictionary containing labels for the uploaded model. Defaults
            to None.
          isOptional: true
          parameterType: STRUCT
        location:
          description: The region for the Google Cloud Project.
          parameterType: STRING
        model_name:
          description: The name for the model in the model registry.
          parameterType: STRING
        name_bucket:
          parameterType: STRING
        path_bucket:
          parameterType: STRING
        project:
          description: The Google Cloud Project ID.
          parameterType: STRING
        serving_container_image_uri:
          description: The URI of the serving container image.
          parameterType: STRING
    outputDefinitions:
      parameters:
        Output:
          parameterType: STRING
deploymentSpec:
  executors:
    exec-create-custom-predict:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - create_custom_predict
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'google-cloud-aiplatform[prediction]==1.33.1'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef create_custom_predict(project         : str,\n              \
          \            location        : str,\n                          name_bucket\
          \     : str,\n                          labels          : Dict)-> str:\n\
          \n    #==== Define BASE_IMAGE ====#\n    repo_name  = 'repo-'+list(labels.values())[0]\n\
          \    container  = list(labels.values())[0]+'-cust-pred'\n    BASE_IMAGE\
          \ = '{}-docker.pkg.dev/{}/{}/{}:latest'.format(location, project, repo_name,\
          \ container)\n\n    #==== Build the Custom Predict Routine ====#\n    from\
          \ pipeline.prod_modules import build_custom_predict_routine_image\n    out,\
          \ err = build_custom_predict_routine_image(BASE_IMAGE             = BASE_IMAGE,\n\
          \                                                  CUST_PRED_ROUTINE_PATH\
          \ = \"pipeline/custom_prediction\",\n                                  \
          \                PROJECT_ID             = project,\n                   \
          \                               NAME_BUCKET            = name_bucket)\n\n\
          \    print('El out es: ' + str(out))\n    print('El err es: ' + str(err))\n\
          \n    return BASE_IMAGE\n\n"
        image: us-east4-docker.pkg.dev/astral-reef-391421/repo-mle-template/mle-template:latest
        resources:
          cpuLimit: 1.0
          memoryLimit: 4.0
    exec-get-data:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - get_data
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef get_data(dataset : OutputPath(\"Dataset\")):\n\n    #==== Importing\
          \ necessary libraries ====#\n    import pandas as pd\n    from sklearn.datasets\
          \ import load_iris\n\n    #==== Loading the Iris dataset ====#\n    iris\
          \ = load_iris()\n    data = pd.DataFrame(data=iris['data'], columns=iris['feature_names'])\n\
          \    data['target'] = iris['target']\n\n    # Define a mapping dictionary\
          \ for the columns\n    column_mapping = {\n        'sepal length (cm)':\
          \ 'sepal_length_cm',\n        'sepal width (cm)': 'sepal_width_cm',\n  \
          \      'petal length (cm)': 'petal_length_cm',\n        'petal width (cm)':\
          \ 'petal_width_cm',\n    }\n\n    # Rename the columns using the mapping\n\
          \    data.rename(columns=column_mapping, inplace=True)\n\n    #==== Save\
          \ the df in GCS ====#\n    from CustomLib.gcp import cloudstorage\n    cloudstorage.write_csv(df\
          \       = data, \n                           gcs_path = dataset + '.csv')\n\
          \n"
        image: us-east4-docker.pkg.dev/astral-reef-391421/repo-mle-template/mle-template:latest
        resources:
          cpuLimit: 1.0
          memoryLimit: 4.0
    exec-split-data:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - split_data
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef split_data(data_input      : InputPath(\"Dataset\"),\n      \
          \         scaler          : Output[Model],\n               data_train  \
          \    : OutputPath(\"Dataset\"),\n               data_test       : OutputPath(\"\
          Dataset\"),):\n\n    #==== Read input data from GCS ====#\n    from CustomLib.gcp\
          \ import cloudstorage\n    data = cloudstorage.read_csv_as_df(gcs_path =\
          \ data_input + '.csv')\n\n    #==== Preprocessing the data ====#\n    from\
          \ src.utils import preprocess_data\n    train_df, test_df, scaler_model\
          \ = preprocess_data(data)\n\n    #==== Save the df in GCS ====#\n    cloudstorage.write_csv(df\
          \       = train_df, \n                           gcs_path = data_train +\
          \ '.csv')\n    cloudstorage.write_csv(df       = test_df, \n           \
          \                gcs_path = data_test + '.csv')\n\n    #==== Save the scaler\
          \ in GCS ====#\n    cloudstorage.write_pickle(model    = scaler_model, \n\
          \                              gcs_path = scaler.path + '/scaler.pkl')\n\
          \n"
        image: us-east4-docker.pkg.dev/astral-reef-391421/repo-mle-template/mle-template:latest
        resources:
          cpuLimit: 1.0
          memoryLimit: 4.0
    exec-testing-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - testing_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef testing_model(model           : Input[Model],\n             \
          \     data_test       : InputPath(\"Dataset\"),\n                  metrics\
          \         : Output[Metrics])->float:\n\n    #==== Read test data from GCS\
          \ ====#\n    from CustomLib.gcp import cloudstorage\n    test_df = cloudstorage.read_csv_as_df(gcs_path\
          \ = data_test + '.csv')\n\n    #==== Read model artifact ====#\n    model\
          \ = cloudstorage.read_pickle(gcs_path = model.path + '/model.pkl')\n\n \
          \   #==== Evaluating the model ====#\n    X_test = test_df.drop(columns=['target'])\n\
          \    y_test = test_df['target']\n    predictions = model.predict(X_test)\n\
          \n    #==== Getting metrics ====#\n    from src.utils import get_metrics\n\
          \    logs, conf_matrix = get_metrics(y_test, predictions)\n\n    for metric,value\
          \ in logs.items():\n        metrics.log_metric(metric, value)\n\n    return\
          \ logs['f1']\n\n"
        image: us-east4-docker.pkg.dev/astral-reef-391421/repo-mle-template/mle-template:latest
        resources:
          cpuLimit: 1.0
          memoryLimit: 4.0
    exec-train-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - train_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef train_model(name_bucket     : str,\n                path_bucket\
          \     : str,\n                data_train      : InputPath(\"Dataset\"),\n\
          \                scaler          : Input[Model],\n                model\
          \           : Output[Model],\n                metrics         : Output[Metrics]):\n\
          \n    #==== Read train data from GCS ====#\n    from CustomLib.gcp import\
          \ cloudstorage\n    train_df = cloudstorage.read_csv_as_df(gcs_path = data_train\
          \ + '.csv')\n\n    #==== Save train data for monitoring ====#\n    file_name\
          \ = \"last_training_path.txt\"\n    gcs_path = f\"gs://{name_bucket}/{path_bucket}/{file_name}\"\
          \n    with open(file_name, 'w') as file:\n        file.write(data_train\
          \ + '.csv')\n    cloudstorage.write_text(text_path = file_name, \n     \
          \                       gcs_path  = gcs_path)\n\n    #==== Training the\
          \ model ====#\n    from src.utils import train_model\n    # Separating features\
          \ and target for training\n    X_train = train_df.drop(columns=['target'])\n\
          \    y_train = train_df['target']\n\n    model_iris = train_model(X_train,\
          \ y_train)\n\n    #==== Evaluating the model ====#\n    predictions = model_iris.predict(X_train)\n\
          \n    #==== Getting metrics ====#\n    from src.utils import get_metrics\n\
          \    log, conf_matrix = get_metrics(y_train, predictions)\n\n    for metric,value\
          \ in log.items():\n        metrics.log_metric(metric, value)\n\n    #====\
          \ Read scaler artifact ====#\n    scaler = cloudstorage.read_pickle(gcs_path\
          \ = scaler.path + '/scaler.pkl')\n\n    #==== Save the model and scaler\
          \ as a .pkl file ====#\n    cloudstorage.write_pickle(model    = model_iris,\
          \ \n                              gcs_path = model.path + '/model.pkl')\n\
          \    cloudstorage.write_pickle(model    = scaler, \n                   \
          \           gcs_path = model.path + '/scaler.pkl')\n\n"
        image: us-east4-docker.pkg.dev/astral-reef-391421/repo-mle-template/mle-template:latest
        resources:
          cpuLimit: 1.0
          memoryLimit: 4.0
    exec-upload-to-model-registry:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - upload_to_model_registry
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'google-cloud-aiplatform==1.29.0'\
          \ 'google-auth==2.17.3' 'google-auth-oauthlib==0.4.6' 'google-auth-httplib2==0.1.0'\
          \ 'google-api-python-client==1.8.0' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef upload_to_model_registry(project                     : str,\n\
          \                             location                    : str,\n     \
          \                        name_bucket                 : str,\n          \
          \                   path_bucket                 : str,\n               \
          \              model_name                  : str,\n                    \
          \         serving_container_image_uri : str,\n                         \
          \    input_model                 : Input[Model],\n                     \
          \        credential_dict             : Dict = None,\n                  \
          \           description                 : str  = None,\n               \
          \              labels                      : Dict = None,)->str:\n    \"\
          \"\"\n    Upload a trained model to the Google Cloud AI Platform's Model\
          \ Registry, creates a version of the model, and returns the uploaded model's\
          \ name.\n\n    Args:\n        project (str)                     : The Google\
          \ Cloud Project ID.\n        location (str)                    : The region\
          \ for the Google Cloud Project.\n        model_name (str)              \
          \    : The name for the model in the model registry.\n        serving_container_image_uri\
          \ (str) : The URI of the serving container image.\n        description (str,\
          \ optional)       : A description for the uploaded model. Defaults to None.\n\
          \        labels (Dict, optional)           : A dictionary containing labels\
          \ for the uploaded model. Defaults to None.\n        credential_dict (Dict)\
          \            : A dictionary containing the service account credentials.\n\
          \        input_model (Input[Model])        : The trained model to be uploaded.\n\
          \n    Returns:\n        (str): The name of the uploaded model in the Google\
          \ Cloud AI Platform's Model Registry.\n    \"\"\"\n    #=== Get the correct\
          \ artifact_uri for the model ===#\n    artifact_uri = input_model.path+'/'\n\
          \n    #=== Generate a timestamp ===#\n    from datetime import datetime\n\
          \    timestamp =datetime.now().strftime(\"%Y%m%d%H%M%S\")\n\n    #=== Initialize\
          \ the aiplatform with the credentials ===#\n    from google.cloud import\
          \ aiplatform\n    from google.oauth2 import service_account\n    from google.auth\
          \ import default\n\n    if credential_dict:\n        # Initialize AI Platform\
          \ with service account credentials from dict\n        credentials = service_account.Credentials.from_service_account_info(credential_dict)\n\
          \    else:\n        # Use default credentials\n        credentials, _ =\
          \ default()\n\n    aiplatform.init(project=project, location=location, credentials=credentials)\n\
          \n    #=== Check if exist a previous version of the model ===#\n    model_list=aiplatform.Model.list(filter\
          \ = 'display_name=\"{}\"'.format(model_name))\n    if len(model_list)>0:\n\
          \        parent_model_name = model_list[0].name\n    else:\n        parent_model_name\
          \ = None\n\n    staging_bucket = f\"gs://{name_bucket}/{path_bucket}/model-registry/{model_name}-{timestamp}\"\
          \n    #=== Upload the model to Model Registry ===#\n    model = aiplatform.Model.upload(display_name\
          \                    = model_name,\n                                   \
          \ artifact_uri                    = artifact_uri,\n                    \
          \                parent_model                    = parent_model_name,\n\
          \                                    description                     = description,\n\
          \                                    labels                          = labels,\n\
          \                                    serving_container_image_uri     = serving_container_image_uri,\n\
          \                                    version_aliases                 = [model_name+'-'+timestamp],\
          \        \n                                    serving_container_health_route\
          \  = \"/v1/models\",\n                                    serving_container_predict_route\
          \ = \"/v1/models/predict\",\n                                    staging_bucket\
          \                  = staging_bucket)\n\n    model.wait()\n\n    import time\n\
          \    time.sleep(300)\n\n    return model.name\n\n"
        image: python:3.9
        resources:
          cpuLimit: 1.0
          memoryLimit: 4.0
pipelineInfo:
  name: mlops-mle-template
root:
  dag:
    outputs:
      artifacts:
        testing-model-metrics:
          artifactSelectors:
          - outputArtifactKey: metrics
            producerSubtask: testing-model
        train-model-metrics:
          artifactSelectors:
          - outputArtifactKey: metrics
            producerSubtask: train-model
    tasks:
      condition-1:
        componentRef:
          name: comp-condition-1
        dependentTasks:
        - testing-model
        - train-model
        inputs:
          artifacts:
            pipelinechannel--train-model-model:
              taskOutputArtifact:
                outputArtifactKey: model
                producerTask: train-model
          parameters:
            pipelinechannel--labels:
              componentInputParameter: labels
            pipelinechannel--location:
              componentInputParameter: location
            pipelinechannel--model_description:
              componentInputParameter: model_description
            pipelinechannel--model_name:
              componentInputParameter: model_name
            pipelinechannel--name_bucket:
              componentInputParameter: name_bucket
            pipelinechannel--path_bucket:
              componentInputParameter: path_bucket
            pipelinechannel--project:
              componentInputParameter: project
            pipelinechannel--testing-model-Output:
              taskOutputParameter:
                outputParameterKey: Output
                producerTask: testing-model
        taskInfo:
          name: model-upload-condition
        triggerPolicy:
          condition: inputs.parameter_values['pipelinechannel--testing-model-Output']
            > 0.96
      get-data:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-get-data
        taskInfo:
          name: Get Data
      split-data:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-split-data
        dependentTasks:
        - get-data
        inputs:
          artifacts:
            data_input:
              taskOutputArtifact:
                outputArtifactKey: dataset
                producerTask: get-data
        taskInfo:
          name: Split Data
      testing-model:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-testing-model
        dependentTasks:
        - split-data
        - train-model
        inputs:
          artifacts:
            data_test:
              taskOutputArtifact:
                outputArtifactKey: data_test
                producerTask: split-data
            model:
              taskOutputArtifact:
                outputArtifactKey: model
                producerTask: train-model
        taskInfo:
          name: Testing Model
      train-model:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-train-model
        dependentTasks:
        - split-data
        inputs:
          artifacts:
            data_train:
              taskOutputArtifact:
                outputArtifactKey: data_train
                producerTask: split-data
            scaler:
              taskOutputArtifact:
                outputArtifactKey: scaler
                producerTask: split-data
          parameters:
            name_bucket:
              componentInputParameter: name_bucket
            path_bucket:
              componentInputParameter: path_bucket
        taskInfo:
          name: Train Model
  inputDefinitions:
    parameters:
      labels:
        parameterType: STRUCT
      location:
        parameterType: STRING
      model_description:
        parameterType: STRING
      model_name:
        parameterType: STRING
      name_bucket:
        parameterType: STRING
      path_bucket:
        parameterType: STRING
      project:
        parameterType: STRING
  outputDefinitions:
    artifacts:
      testing-model-metrics:
        artifactType:
          schemaTitle: system.Metrics
          schemaVersion: 0.0.1
      train-model-metrics:
        artifactType:
          schemaTitle: system.Metrics
          schemaVersion: 0.0.1
schemaVersion: 2.1.0
sdkVersion: kfp-2.7.0
